# -*- coding: utf-8 -*-
"""simple_DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w6x1QSVSWY5OL_Lr9JHbD6zhzIVcD2Tc
"""

!pip install gym[box2d]

import torch
import torch.nn as nn
import torch.nn.functional as f
import torch.optim as optim 
import numpy as np
import matplotlib.pyplot as plt 
import gym

class DeepQnetwork(nn.Module):

  def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):
    
    super(DeepQnetwork, self).__init__()
    self.input_dims = input_dims # 8 elemtns of the observation vetor 
    self.fc1_dims = fc2_dims
    self.fc2_dims =  fc2_dims
    self.n_actions = n_actions
    self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)
    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)
    self.fc3 = nn.Linear(self.fc2_dims, n_actions)
    self.optimizer = optim.Adam(self.parameters(), lr = lr)
    self.loss = nn.MSELoss()
    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    self.to(self.device) # send the netork to the device which is probably a gpu 


  def forward(self, state):

    x = F.relu(self.fc1(state))
    x = F.relu(self.fc2(x))
    actions = self.fc3(x) # no need for sigmoid function for 

    return actions 


## define the agent 
## parameters include

# gamma = discount factor for the bellman equation 
# epsilon = exploration or explitation parameter 
# learning rate = learning rate for the agent 
# input_dims = number of dimensions used to represent a state 
# batch_size = number of memory transitions considered when doing an update of the DQN model 
# max_mem_size = number of states that can be stored 
# eps_end = lowest epsilon value 
# epsilon decrimental value 

# use deque for storing the memeory or the epxerienced replay transitions 

class Agent():  

  def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, 
               max_mem_size = 100000, eps_end = 0.01, eps_dec = 5e-4):
    
    self.gamma = gamma
    self.epsilon = epsilon 
    self.eps_min = eps_end
    self.eps_dec = eps_dec
    self.lr = lr # agent learning rate for the DQN models 
    self.action_space = [i for i in range(n_actions)] # integer representation of the actions to be used in epsilon greedy action selection
    self.batch_size = batch_size 
    self.mem_cntr = 0 # keep track of the first available memory spot 
    self.mem_size = max_mem_size 

    self.Q_eval = DeepQnetwork(self.lr, n_actions = n_actions, input_dims = input_dims, 
                               fc1_dims = 256, fc2_dims = 256) # action value function network (estimates/predicts the action values)
    
    self.state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32) # storing the current state 
    self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32) # storing the next state 
    self.action_memory = np.zeros(self.mem_size, dtype = np.int32) # a record of the actions the agent took 
    self.reward_memory = np.zeros(self.mem_size,dtype = np.float32) # record of the rewards it collects 
    self.terminal_memory = np.zeros(self.mem_size, dtype = np.bool) # final state # used as a mask for setting te value of the next state to be zero 


# need an interface function for the environment to help the agent store the states and rewards within it 

# state = current state 
# action = action taken
# new_state = next state 
# done = is the object done flying

  def store_transition(self, state,action, reward,  new_state, done):

    # what is the positon of the first unoccupied memory

    index = self.mem_cntr % self.mem_size # where to store it 
    # now we store it 

    self.state_memory[index] = state 
    self.new_state_memory[index] = new_state
    self.reward_memory[index] = reward
    self.terminal_memory[index] = done 

    self.mem_cntr += 1  # filled up a memory spot 

  # agent should have a function to choose an action 

  def choose_action(self, observation):
    if np.random.random > self.epsilon:
      state = torch.Tensor([observation]).to(self.Q_eval.device) # send he new observation to the neural net
      actions = self.Q_eval.forward(state) # pass the input through the network 
      action = torch.argmax(actions).item() # take the maximum of the output, which is an integer position 
    else:
      action = np.random.choice(self.action_space)

    return action 

# write code for td learning ( back prop for the deep q network)
# note - it is important to know how and when a function is going to be called in an entire process

  def learn(self):

    if self.mem_cntr < self.batch_size:
      return 

    self.Q_eval.optimizer.zero_grad()

    max_mem = min(self.mem_cntr, self.mem_size) # only select the minimum  between the mem counter and the memory size 
    batch = np.random.choice(max_mem, self.batch_size, replace = False) # genrates an array for batch indexing 
    batch_index = np.arange(self.batch_size, dtype = np.int32) # generate the indexes for the batch 

    state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_eval.device)
    new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)
    reward_batch = torch.tensor(self.reward_memory[batch]).to(self.Q_eval.device)
    terminal_batch = torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)
    action_batch = self.action_memory[batch]


    q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch] # action value function of the current state 
    q_next = self.Q_eval.forward(new_state_batch) # action value function of the next state 
    q_next[terminal_batch] = 0.0 # using the terminal_batch indices, make those states equal to zero, rather than having a number 

    q_target = reward_batch + self.gamma * torch.max(q_next, dim = 1)[0] # max function return the max value as well as the index, but we only want the value 

    loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)
    loss.backward()
    self.Q_eval.optimizer.step()

    self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \
                    else self.eps_min

def plotLearning(x, scores, epsilons, filename, lines=None):

  fig=plt.figure()
  ax=fig.add_subplot(111, label="1")
  ax2=fig.add_subplot(111, label="2", frame_on=False)

  ax.plot(x, epsilons, color="C0")
  ax.set_xlabel("Game", color="C0")
  ax.set_ylabel("Epsilon", color="C0")
  ax.tick_params(axis='x', colors="C0")
  ax.tick_params(axis='y', colors="C0")

  N = len(scores)
  running_avg = np.empty(N)
  for t in range(N):
    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])

  ax2.scatter(x, running_avg, color="C1")
  #ax2.xaxis.tick_top()
  ax2.axes.get_xaxis().set_visible(False)
  ax2.yaxis.tick_right()
  #ax2.set_xlabel('x label 2', color="C1")
  ax2.set_ylabel('Score', color="C1")
  #ax2.xaxis.set_label_position('top')
  ax2.yaxis.set_label_position('right')
  #ax2.tick_params(axis='x', colors="C1")
  ax2.tick_params(axis='y', colors="C1")

  if lines is not None:
      for line in lines:
          plt.axvline(x=line)

  plt.savefig(filename)

## main training loop 


env = gym.make('LunarLander')
agent = Agent(gamma = 0.99, epsilon = 1.0, batch_size = 64, n_actions = 4, eps_end = 0.01, input_dims = [8], lr = 0.003)
scores, eps_history = [], []
n_episodes = 500

# n_epipsodes = number of episodes 

for i in range(n_episodes):

  score = 0 # total reward for this episode 
  done = False 
  observation = env.reset() # reset the environment to the initial state  
  
  while not done:
    action = agent.choose_action(observation)
    observation_, reward, done, info = env.step(action) # observation_ = next state 
    score += reward 
    agent.store_transition(observation,action, reward, observation_, done) # store the current state, the next state 

    agent.learn() # learn from the stored transition 
    observation = observation_ # old observation is the new observation 
  
  scores.append(score)
  eps_history.append(agent.epsilon)

  avg_score = np.mean(scores[-100:])

  print('episode', i, 'score %.2f' % score, 'average score %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)

x = [i + 1 for i in range(n_games)]
file_name = 'lunar_lander_2020.png'
plotLearning(x, scores, eps_history, file_name)